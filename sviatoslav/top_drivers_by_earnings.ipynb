{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Топ водіїв за прибутком із поїздок, відстань яких перевищує 10 миль, за останній тиждень\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "your 131072x1 screen size is bogus. expect trouble\n",
      "25/04/16 13:40:14 WARN Utils: Your hostname, DESKTOP-DP2FLCF resolves to a loopback address: 127.0.1.1; using 172.22.104.161 instead (on interface eth0)\n",
      "25/04/16 13:40:14 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/04/16 13:40:16 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "25/04/16 13:40:17 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "25/04/16 13:40:17 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.\n",
      "25/04/16 13:40:17 WARN Utils: Service 'SparkUI' could not bind on port 4042. Attempting port 4043.\n",
      "25/04/16 13:40:17 WARN Utils: Service 'SparkUI' could not bind on port 4043. Attempting port 4044.\n",
      "25/04/16 13:40:17 WARN Utils: Service 'SparkUI' could not bind on port 4044. Attempting port 4045.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, expr\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"SimpleApp\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_fare_init = spark.read.option(\"header\", True).option(\"inferSchema\", True).csv(\"../data_lake/trip_fare_1.csv\")\n",
    "df_trip_init = spark.read.option(\"header\", True).option(\"inferSchema\", True).csv(\"../data_lake/trip_data_1.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "for column in df_fare_init.columns:\n",
    "    df_fare_init = df_fare_init.withColumnRenamed(column, column.strip().replace(\" \", \"_\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import to_date, lit\n",
    "\n",
    "df_trip = df_trip_init.filter(\n",
    "    (col(\"passenger_count\") > 0) & (col(\"passenger_count\") <= 10) &\n",
    "    (col(\"trip_time_in_secs\") > 0) &\n",
    "    (col(\"trip_distance\") > 0) &\n",
    "    (col(\"pickup_latitude\").between(40.5, 41.0)) &\n",
    "    (col(\"dropoff_latitude\").between(40.5, 41.0)) &\n",
    "    (col(\"pickup_longitude\").between(-74.5, -73.0)) &\n",
    "    (col(\"dropoff_longitude\").between(-74.5, -73.0)) &\n",
    "    (col(\"pickup_datetime\") <= lit(\"2013-01-27\"))\n",
    ")\n",
    "\n",
    "df_fare = df_fare_init.filter(\n",
    "    (col(\"fare_amount\") >= 0) &\n",
    "    (col(\"tip_amount\") >= 0) &\n",
    "    (col(\"tolls_amount\") >= 0) &\n",
    "    (col(\"total_amount\") >= 0) &\n",
    "    ((col(\"fare_amount\") > 0) | (col(\"payment_type\") == \"NOC\")) &\n",
    "    (col(\"pickup_datetime\") <= lit(\"2013-01-27\"))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import max, min, col, to_date, trunc\n",
    "\n",
    "df = df_trip.join(df_fare, on=[\"medallion\", \"hack_license\", \"pickup_datetime\", \"vendor_id\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, to_date, date_sub, lit, sum as Fsum, round as Fround, count\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "week_start = date_sub(lit(\"2013-01-27\"), 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_last_week = df.withColumn(\"trip_date\", to_date(\"pickup_datetime\")) \\\n",
    "    .filter((col(\"trip_date\") >= week_start) & (col(\"trip_distance\") > 10))\n",
    "\n",
    "df_top_drivers = df_last_week.groupBy(\"hack_license\").agg(\n",
    "    Fround(Fsum(\"total_amount\"), 2).alias(\"total_earnings\"),\n",
    "    count(\"*\").alias(\"long_trip_count\")\n",
    ").orderBy(col(\"total_earnings\").desc())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 8:=============>                                           (3 + 10) / 13]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------+--------------+---------------+\n",
      "|hack_license                    |total_earnings|long_trip_count|\n",
      "+--------------------------------+--------------+---------------+\n",
      "|3895ECEDAF11644ADDDCCFB57F10A205|1730.7        |31             |\n",
      "|4C41D9F8071DC7FA9A526ADACD6DAD77|1695.15       |31             |\n",
      "|D379EFF4BC03AD70DAB2729A1A0CFF21|1658.64       |31             |\n",
      "|D92C6162E7E82E115E75A8CFDD435FE3|1604.4        |27             |\n",
      "|6B5C69870E8775E65570B7CF0B4B41CC|1600.35       |28             |\n",
      "|5114DF85775775ED4F53235D8478E80B|1594.85       |28             |\n",
      "|5682C07954E9B555F3809EB59CA3EA51|1579.95       |28             |\n",
      "|E77D8148C5C74128D72F3FEEA85E93F1|1575.62       |23             |\n",
      "|6EC4BD844C40B9A941D4F67DC926E453|1574.7        |28             |\n",
      "|6CD44E75DB4B2E74D44717F6C7FB566C|1555.22       |27             |\n",
      "|59EE2CA458BC2BA9A5F1452E5531F06A|1497.43       |25             |\n",
      "|76801FA0D6F094C97FE8716AB64FFF41|1495.65       |27             |\n",
      "|8FC3DA1352B409763A90F49F26069DA6|1427.3        |24             |\n",
      "|A091DF8DEA11A37E9EEA395BD658E123|1381.5        |22             |\n",
      "|5ACC63C350269BE35CCE34B2400B59B6|1356.9        |24             |\n",
      "|318651E071D292B217ED1635719C7E90|1355.69       |27             |\n",
      "|970978A9CF1C59F8DC462176E305EFCD|1346.07       |23             |\n",
      "|0EF3E168BC0B685185E3EA6A37A49723|1335.18       |27             |\n",
      "|4A913C5964F0F0C1D9049F2F57856C07|1330.65       |24             |\n",
      "|97191A64A3C40CFD2584901BDF5FC835|1329.5        |21             |\n",
      "+--------------------------------+--------------+---------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_top_drivers.show(20, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_top_drivers.coalesce(1).write.mode(\"overwrite\").option(\"header\", True).csv(\"./results/top_drivers_by_earnings.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
